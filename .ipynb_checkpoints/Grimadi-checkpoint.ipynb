{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is a reproduce of Grimadi et al. 2020 RSE - \"Flood mapping under vegetation using single SAR acquisitions\" [link](https://www.sciencedirect.com/science/article/pii/S0034425719306029?via%3Dihub#bb0510)\n",
    "\n",
    "We use GEE to process the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "import numpy as np\n",
    "import ee\n",
    "from pysheds.grid import Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2438c8918348719b1292d89e915664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[40, -100], controls=(WidgetControl(options=['position'], widget=HBox(children=(ToggleButton(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Map= geemap.Map(center=[40,-100], zoom=4)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "The data we need to reproduce this result:\n",
    "\n",
    "- SAR backscatter data HH polarized (Sentinel-1 IW mode)\n",
    "- DEM (SRTM)\n",
    "- Land Cover (Copernicus Global Land Cover Layers: CGLS-LC100 collection 2)\n",
    "- Open water observation (JRC Monthly Water History, v1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#study area\n",
    "region_shp= '/Users/allen/OneDrive - University of Oklahoma/CRESTHH/Case study - Harvey/Houston_basin.shp'\n",
    "region= geemap.shp_to_ee(region_shp)\n",
    "Map.addLayer(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR= ee.Image('COPERNICUS/S1_GRD/S1B_IW_GRDH_1SDV_20170830T122203_20170830T122232_007169_00CA2C_E7BF').clip(region)\n",
    "Map.addLayer(SAR.select('VH'), {'min':-30, 'max':5}, 'SAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "land= ee.ImageCollection(\"COPERNICUS/Landcover/100m/Proba-V/Global\").select('discrete_classification').mosaic().clip(region)\n",
    "# land= land.map(lambda image: image.clip(region))\n",
    "\n",
    "Map.addLayer(land,  {'min': 0.0,\n",
    "  'max': 200.0,\n",
    "  'palette': [\n",
    "    \"032f7e\",\"02740b\",\"02740b\",\"8cf502\",\"8cf502\",\"a4da01\",\n",
    "    \"ffbd05\",\"ffbd05\",\"7a5a02\",\"f0ff0f\",\"869b36\",\"6091b4\",\n",
    "    \"999999\",\"ff4e4e\",\"ff4e4e\",\"ffffff\",\"feffc0\",\"020202\",\n",
    "    \"020202\"]},\n",
    "  name='landcover' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "water= ee.Image('JRC/GSW1_0/GlobalSurfaceWater').clip(region)\n",
    "Map.addLayer(water, {\n",
    "  'bands': ['occurrence'],\n",
    "  'min': 0.0,\n",
    "  'max': 100.0,\n",
    "  'palette': ['ffffff', 'fffcb8', '0905ff']\n",
    "}, 'water')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating URL ...\n",
      "Downloading data from https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/d4deaa11daf092262cb28a6b5411ac55-1a0fd21e77d913292748d2d3c58f5621:getPixels\n",
      "Please wait ...\n",
      "Data downloaded to /Users/allen/Documents/Python/FloodDetection/data\n"
     ]
    }
   ],
   "source": [
    "# geemap.ee_export_image(land, filename='data/landcover.tif', region=region.geometry(), scale=90, file_per_band=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Credit: Ben Lewis\"\n",
    "import numpy as np\n",
    "import scipy.optimize, scipy.stats\n",
    "\n",
    "def fuzzy_smf(x, a, b):\n",
    "    \"\"\"\n",
    "    Fuzzy step function\n",
    "    \n",
    "    Examples:\n",
    "    >>> x = np.arange(7)\n",
    "    >>> fuzzy_smf(x, 3, 5).tolist()\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0]\n",
    "    >>> fuzzy_zmf(x, 3, 5).tolist()\n",
    "    [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0]\n",
    "    \"\"\"\n",
    "    delta = b - a\n",
    "    middle = np.mean([a, b])\n",
    "    \n",
    "    A = 2 * ((x - a) / delta)**2\n",
    "    B = 2 * ((x - b) / delta)**2\n",
    "    \n",
    "    result = np.where(x < middle, A, 1 - B)\n",
    "    result[x < a] = 0\n",
    "    result[x > b] = 1\n",
    "    return result\n",
    "\n",
    "def fuzzy_zmf(x, a, b):\n",
    "    return 1 - fuzzy_smf(x, a, b)\n",
    "\n",
    "def hist_fixedwidth(population, binwidth=0.1):\n",
    "    \"\"\"\n",
    "    Return (frequency, centres) for a histogram with uniform-width bins\n",
    "    \n",
    "    >>> freq, centres = hist_fixedwidth(np.asarray([5, 9.5, 10.5, 6, 6, 6, 6, 6]), binwidth=2)\n",
    "    >>> centres.tolist()\n",
    "    [6.0, 8.0, 10.0]\n",
    "    >>> freq.tolist() # 6/8 per 2; 0 per 2; 2/8 per 2\n",
    "    [0.375, 0.0, 0.125]\n",
    "    \"\"\"\n",
    "    # Could replace .max() with 0dB to discard non-negative samples?\n",
    "    edges = np.arange(start=population.min(), stop=population.max() + binwidth, step=binwidth)\n",
    "    counts, edges = np.histogram(population, bins=edges, density=True)\n",
    "    centres = (edges[:-1] + edges[1:]) / 2\n",
    "    return counts, centres\n",
    "\n",
    "def find_mode(population):\n",
    "    \"\"\"\n",
    "    Returns estimate of distribution mode.\n",
    "    \n",
    "    >>> round(find_mode(np.asarray([-10.05, 20.31, 20.29, 17, 20.27, 17, 37])), 1)\n",
    "    20.3\n",
    "    \"\"\"\n",
    "    counts, values = hist_fixedwidth(population)\n",
    "    return values[counts.argmax()]\n",
    "\n",
    "def leftFitNormal(population):\n",
    "    \"\"\"\n",
    "    Obtain mode and standard deviation from the left side of a population.\n",
    "    >>> mode, sigma = leftFitNormal(np.random.normal(loc=-20, scale=3, size=10000))\n",
    "    >>> -22 < mode < -18\n",
    "    True\n",
    "    >>> round(sigma)\n",
    "    3.0\n",
    "    \"\"\"\n",
    "    # Quick alternative robust (but symmetric) fit:\n",
    "    #median = np.nanmedian(population) \n",
    "    #MADstd = np.nanmedian(np.abs(population - median)) * 1.4826 # robust estimator\n",
    "    \n",
    "    std = np.nanstd(population) # naive initial estimate\n",
    "\n",
    "    Y, X = hist_fixedwidth(population)\n",
    "    \n",
    "    # Take left side of distribution\n",
    "    pos = Y.argmax()\n",
    "    mode = X[pos]\n",
    "    X = X[:pos+1]\n",
    "    Y = Y[:pos+1]\n",
    "    \n",
    "    # fit gaussian to (left side of) distribution\n",
    "    def gaussian(x, mean, sigma):\n",
    "        return np.exp(-0.5 * ((x - mean)/sigma)**2) / (sigma * (2*np.pi)**0.5)\n",
    "    (mean, std), cov = scipy.optimize.curve_fit(gaussian, X, Y, p0=[mode, std])\n",
    "    \n",
    "    return mode, std\n",
    "\n",
    "def leftFitNormal2(population):\n",
    "    \"\"\"\n",
    "    Obtain mode, right 1/20 maximum, and covariance of left fit.\n",
    "    >>> mode, m20, measure = leftFitNormal2(np.random.normal(loc=-20, scale=3, size=5000))\n",
    "    >>> m20 > -17\n",
    "    True\n",
    "    >>> 1 >= measure > 0.9\n",
    "    True\n",
    "    >>> mode, m20, measure = leftFitNormal2(np.random.normal(loc=-20, scale=3, size=5000)**2)\n",
    "    >>> measure > 0.9\n",
    "    False\n",
    "    \"\"\"\n",
    "    std = np.nanstd(population) # naive initial estimate\n",
    "\n",
    "    Y, X = hist_fixedwidth(population)\n",
    "    \n",
    "    # Find the peak\n",
    "    pos = Y.argmax()\n",
    "    mode = X[pos]\n",
    "    \n",
    "    # Perform a one-sided fit (for standard deviation and with mode held fixed)\n",
    "    def gaussian(x, sigma):\n",
    "        return np.exp(-0.5 * ((x - mode)/sigma)**2) / (sigma * (2*np.pi)**0.5)\n",
    "    std, cov = scipy.optimize.curve_fit(gaussian, X[:pos+1], Y[:pos+1], p0=std) # histogram fit\n",
    "    \n",
    "    # Calculate two-sided covariance of the fit\n",
    "    double = min(len(X), 2*pos)\n",
    "    correlation = np.corrcoef(gaussian(X[:double], std), Y[:double])[0, 1] # normalised covariance\n",
    "    \n",
    "    # Find the right intercept with 1/20th maximum\n",
    "    pos20 = (abs(Y[pos:] - Y.max() / 20)).argmin()\n",
    "    m20 = X[pos + pos20]\n",
    "    \n",
    "    return mode, m20, correlation\n",
    "\n",
    "def leftFitGamma(population): # TODO: test this function\n",
    "    \"\"\"\n",
    "    Left fit of Gamma distribution, to extract mode.\n",
    "    \n",
    "    Assumes population is clipped such that mode is within ten units of the maximum sample.\n",
    "    \n",
    "    #>>>mode = leftFitGamma(numpy.random.gamma(9, (-5 + 20)/(9 - 1), 5000) - 20)\n",
    "    #>>>mode\n",
    "    \"\"\"\n",
    "    Y, X = hist_fixedwidth(population)\n",
    "    x0 = X[0]\n",
    "    candidates = np.arange(X[-1] - 10, X[-1], 0.1)\n",
    "    rmse = np.full_like(candidates, np.nan)\n",
    "    for i, mode in enumerate(modes):\n",
    "        def gamma(x, k):\n",
    "            return scipy.stats.gamma.pdf(x, k, X[0], (mode - X[0])/(k - 1))\n",
    "        # k = nonlinfit\n",
    "        #rmse[i] = \n",
    "    return candidates[rmse.argmin()]\n",
    "        \n",
    "    \n",
    "class Inseparable(Exception): # may be raised by chiSeparate\n",
    "    pass\n",
    "\n",
    "def chiSeparate(control, test, nbins=100):\n",
    "    \"\"\"\n",
    "    Perform separability T-test and probability binning.\n",
    "    \n",
    "    Given two distributions that may overlap in the middle,\n",
    "    try to find end-intervals that separate between the two.\n",
    "    \"\"\"\n",
    "    n0 = len(control)\n",
    "    n1 = len(test)\n",
    "    minsample = min(n0, n1)\n",
    "    if (minsample < 200) or (n0 < nbins):\n",
    "        raise Inseparable\n",
    "    \n",
    "    edges = np.nanquantile(control, np.linspace(0, 1, nbins + 1))\n",
    "    freq0 = 1 / nbins # (constant) relative frequency for control sample\n",
    "    freq1 = np.histogram(test, edges)[0] / n1 # relative frequencies for test sample\n",
    "    \n",
    "    x = (edges[1:] + edges[:-1]) / 2 # the bin centers\n",
    "    \n",
    "    z2 = (freq0 - freq1)**2 / (freq0 + freq1)\n",
    "    zk = (freq0 - freq1) / np.sqrt(freq0/n0 + freq1/n1)\n",
    "    \n",
    "    if (minsample * z2.sum() * nbins**-0.5 - nbins**0.5) < 4:\n",
    "        raise Inseparable\n",
    "    \n",
    "    # find where in the domain that zk (interpolated) crosses -1.96\n",
    "    y = np.poly1d(np.polyfit(centres, zk + 1.96, deg=9))\n",
    "    roots = y.roots.real[y.roots.imag == 0]\n",
    "    roots = roots[(x[0] < roots) & (roots < x[-1])]\n",
    "    \n",
    "    if not len(roots):\n",
    "        return control.min(), 0\n",
    "    else:\n",
    "        return roots.min(), roots.max()\n",
    "\n",
    "def openwater(backscatter, persistent, historic):\n",
    "    \"\"\"\n",
    "    Adaptively select thresholds for classifying areas of open water (low backscatter)\n",
    "    \"\"\"\n",
    "    \n",
    "    dark = backscatter[persistent]\n",
    "    notdark = backscatter[~persistent]\n",
    "    unprecedented = backscatter[~historic]\n",
    "    wettable = backscatter[historic]\n",
    "    \n",
    "    # \"Methodology 1\" - fit left side of normal distribution to persistant waterbodies\n",
    "    L1, std = leftFitNormal(dark)\n",
    "    R1 = L1 + std\n",
    "    \n",
    "    # \"Methodology 2\" - find left-separability of distribution by persistance\n",
    "    try:\n",
    "        R2 = chiSeparate(notdark, dark)[0]\n",
    "    except Inseparable:\n",
    "        R2 = R1\n",
    "    \n",
    "    # \"Methodology 3\" - find left-separability of distribution by precedent\n",
    "    try:\n",
    "        R3 = chiSeparate(unprecedented, wettable)[0]\n",
    "    except Inseparable:\n",
    "        R3 = R1\n",
    "    \n",
    "    mode, std = leftFitNormal(notdark)\n",
    "    upperbound = mode - 0.5 * std\n",
    "    \n",
    "    # Conclusion - decide which thresholds to return\n",
    "    \n",
    "    R = min(R1, R2, R3)\n",
    "    \n",
    "    if R > upperbound: # abort! (no apparent water)\n",
    "        return [backscatter.min()] * 2\n",
    "\n",
    "    L = L1 if R == R1 else np.mean([R, find_mode(dark < R2)])\n",
    "        \n",
    "    if L > R:\n",
    "        if False: #R3 < R2: # TODO\n",
    "            L = leftFitGamma(wettable) # Matgen method\n",
    "        else:\n",
    "            L = np.mean([R, wettable.min()]) # should also require minimum is >-40dB\n",
    "\n",
    "    return L, R\n",
    "\n",
    "\n",
    "def vegetation(backscatter, lowlying, precedent, v4=False):\n",
    "    \"\"\"\n",
    "    Adaptively select thresholds for classifying inundated vegetation (high backscatter)\n",
    "    \n",
    "    Assumes open-water (very low backscatter) has already been filtered out.\n",
    "    \"\"\"\n",
    "    \n",
    "    test = backscatter[lowlying] # mixture wet and dry\n",
    "    control = backscatter[~lowlying] # presume all dry\n",
    "    \n",
    "    S1 = chiSeparate(control, test)[1]\n",
    "    \n",
    "    mode, S2, correlation = leftFitNormal2(control)\n",
    "    \n",
    "    # Portion of samples within [S1, S2]\n",
    "    def portion(x):\n",
    "        return ((x > S1) & (x < S2)).mean()\n",
    "    HO = portion(backscatter[lowlying & precedent])\n",
    "    HNO = portion(backscatter[lowlying & ~precedent])\n",
    "    C = portion(control)\n",
    "    \n",
    "    # Produce gamma exponents, to warp or bias the fuzzy interval\n",
    "    enhance = 4 * (C / HO)**2\n",
    "    attenuate = 4 * (C / HNO)**2\n",
    "    \n",
    "    if v4:\n",
    "        try:\n",
    "            # option for more rigorous consistency\n",
    "            test = backscatter[precedent & lowlying]\n",
    "            control = backscatter[~precedent & ~lowlying]\n",
    "            \n",
    "            S1 = chiSeparate(control, test)[1]\n",
    "        except Inseparable:\n",
    "            S1 = 0 \n",
    "    \n",
    "    return S1, S2, enhance, attenuate, correlation\n",
    "\n",
    "\n",
    "def classify(backscatter, wofs, hand, landcover):\n",
    "    \"\"\"\n",
    "    Generate probabilistic flood raster\n",
    "    \"\"\"\n",
    "    \n",
    "    # define input thresholds\n",
    "    persistent = wofs > 0.8\n",
    "    historic = wofs > 0.001\n",
    "    lowlying = hand < 20\n",
    "    \n",
    "    ow1, ow2 = openwater(backscatter, persistent, historic) # find thresholds\n",
    "    ow = fuzzy_zmf(backscatter, ow1, ow2) # produce open water raster\n",
    "    \n",
    "    notopen = (backscatter > ow2) & ~persistent\n",
    "    \n",
    "    veg = np.zeros_like(backscatter, dtype=np.float32)\n",
    "    \n",
    "    for category in np.unique(landcover):\n",
    "        try:\n",
    "            subset = (landcover == category)\n",
    "            \n",
    "            s = subset & notopen # initially exclude open-water from vegetation analysis\n",
    "            \n",
    "            S1, S2, enhance, attenuate, cov = vegetation(backscatter[s], lowlying[s], historic[s])\n",
    "        \n",
    "            veg[subset] = fuzzy_smf(backscatter[subset], S1, S2)\n",
    "            veg[subset & historic] **= enhance\n",
    "            veg[subset & ~historic] **= attenuate\n",
    "            \n",
    "            # mask strong claims if evidence is uncompelling\n",
    "            if cov > 0.998:\n",
    "                veg[subset & ~lowlying & (backscatter > S2)] = 0\n",
    "                \n",
    "        except Inseparable:\n",
    "            pass\n",
    "\n",
    "    return ow + veg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DEM\n",
    "grid= Grid.from_raster('/Users/allen/OneDrive - University of Oklahoma/CRESTHH/DEM_10m.tif', data_name='DEM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
